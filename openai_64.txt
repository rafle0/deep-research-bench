Enhancing Cascaded PID Control Performance in UAV Flight Controllers
Introduction and Background
Open-source UAV autopilots (e.g. ArduPilot, PX4, Betaflight, Crazyflie) typically use cascaded PID controllers for attitude and position control. In a cascaded architecture, separate PID loops control different levels (rate, attitude angle, velocity, position), with each loop feeding the next inner loop ([Controller Diagrams | PX4 Guide (main)](https://docs.px4.io/main/en/flight_stack/controller_diagrams.html#:~:text=,shown as a)) ([Controllers in the Crazyflie | Bitcraze](https://www.bitcraze.io/documentation/repository/crazyflie-firmware/master/functional-areas/sensor-to-control/controllers/#:~:text=By default%2C the Crazyflie firmware,Ultimately%2C regardless of)). For example, an outer attitude (angle) PID provides a desired angular rate to an inner rate PID, which then drives the motors. This layered approach simplifies tuning and ensures stability by handling fast inner dynamics (rates) separately from slower outer dynamics (angles/position) ([Controllers in the Crazyflie | Bitcraze](https://www.bitcraze.io/documentation/repository/crazyflie-firmware/master/functional-areas/sensor-to-control/controllers/#:~:text=By

(Controllers in the Crazyflie | Bitcraze) Figure 1: A typical cascaded PID control architecture in a quadrotor UAV (Crazyflie). The outer position and velocity PID loops (100 Hz) feed into the inner attitude and attitude-rate PID loops (500 Hz). A state estimator fuses sensor measurements to provide accurate feedback (position, velocity, attitude) to each control loop, while the innermost loop uses raw gyro rates ([Controllers in the Crazyflie | Bitcraze](https://www.bitcraze.io/documentation/repository/crazyflie-firmware/master/functional-areas/sensor-to-control/controllers/#:~:text=By default%2C the Crazyflie firmware,Ultimately%2C regardless of)). This structure allows handling fast attitude dynamics separately from slower position dynamics, but traditionally uses fixed PID gains that must be tuned for a “nominal” flight condition.

However, a major challenge with these fixed-gain PID controllers is maintaining optimal performance across diverse flight conditions. Most off-the-shelf autopilots use linear controllers tuned for specific nominal conditions, meaning their ability to handle large changes in the UAV or environment is limited ([Embedding Adaptive Features in the ArduPilot Control Architecture for Unmanned Aerial Vehicles](https://research.tudelft.nl/files/146358924/Embedding_Adaptive_Features_in_the_ArduPilot_Control_Architecture_for_Unmanned_Aerial_Vehicles.pdf#:~:text=parameters alter the flight dynamics,controllers to avoid loss of)). In practice, users often find that adding payloads, flying in strong winds, or operating at different speeds requires re-tuning the PID gains because the default fixed gains cannot cope with such alterations ([Embedding Adaptive Features in the ArduPilot Control Architecture for Unmanned Aerial Vehicles](https://research.tudelft.nl/files/146358924/Embedding_Adaptive_Features_in_the_ArduPilot_Control_Architecture_for_Unmanned_Aerial_Vehicles.pdf#:~:text=parameters

Limitations of Fixed PID Gains in Diverse Conditions
A fixed set of PID parameters is often a compromise that works “well enough” only in a narrow band of operating conditions. UAV dynamics can vary widely due to factors such as changes in mass and inertia (e.g. deploying a payload or attaching sensors), variations in battery voltage/thrust, aerodynamic effects (wind gusts, ground effect, air density at altitude), and different flight modes. For instance, a quadcopter’s responsiveness will change as the battery drains (reducing available thrust) or if a heavy payload is added, and a fixed PID may then respond too slowly or even saturate. A study on an adaptive capture drone explicitly noted that a basic cascaded PID tuned for one model “has a hard time to adapt to changing conditions” – after the drone caught a target (increasing its mass drastically), the altitude PID could no longer maintain the same performance because the system’s dynamics had severely changed ([A Modified Model Reference Adaptive Controller (M-MRAC) Using an Updated MIT-Rule for the Altitude of a UAV](https://www.mdpi.com/2079-9292/9/7/1104#:~:text=will increase significantly with the,MMRAC) for)). Traditional PID controllers lack any mechanism to adjust their gains when the plant dynamics or environment deviate from the original tuning scenario.

Fixed-wing UAVs highlight this limitation clearly. Their aerodynamic stability and control response vary dramatically with airspeed: at high speeds, control surfaces become more effective (risking oscillations with too-high gains), while at low speeds near stall, the same gains might be too low to maintain control authority. If one were to use fixed gains for all flight phases, the aircraft might either oscillate at cruise or respond sluggishly near stall. As a result, without intervention, fixed PIDs cannot simultaneously be optimal for takeoff, cruise, and landing flight regimes. VTOL aircraft (e.g. tilt-rotors or quadplanes) face an even more pronounced challenge: they transition between hover mode (multirotor-like, low-speed, high angle responsiveness) and forward flight (fixed-wing-like). A single set of PID gains is unlikely to suit both regimes. For example, a tiltrotor’s yaw and pitch controllers might become unstable if hover-tuned gains were applied during fast forward flight, and vice versa ([Gain-Scheduled PID Autotuning a VTOL UAV During Forward and Backward Transition - MATLAB & Simulink ](https://www.mathworks.com/help/slcontrol/ug/gain-scheduled-control-vtol-uav.html#:~:text=This example shows how to,UAV Toolbox)). Indeed, developers often had to manually switch or “re-tune” controllers when a VTOL transitions, illustrating that fixed gains are insufficient across such distinct modes.

Even within one flight mode, environmental disturbances can push fixed-gain controllers to their limits. Wind gusts or turbulent air can introduce rapid disturbances that a PID tuned in calm conditions may not reject quickly enough. Changes in air density (with altitude or temperature) affect lift and motor thrust; a multicopter flying at a high altitude (thin air) might require higher throttle outputs for the same maneuver, potentially demanding different integral gains to avoid steady-state error. Sensor noise or delay can also effectively change how a PID should be tuned (e.g. if GPS updates are slow, a position hold PID might need lower gains to remain stable). In summary, using fixed PID gains means the UAV’s performance and stability will only be optimal around the conditions it was tuned for ([Embedding Adaptive Features in the ArduPilot Control Architecture for Unmanned Aerial Vehicles](https://research.tudelft.nl/files/146358924/Embedding_Adaptive_Features_in_the_ArduPilot_Control_Architecture_for_Unmanned_Aerial_Vehicles.pdf#:~:text=need to minimize the alteration,controllers to avoid loss of)). Outside those conditions, controllers often require continuous manual re-tuning to maintain satisfactory performance ([Embedding Adaptive Features in the ArduPilot Control Architecture for Unmanned Aerial Vehicles](https://research.tudelft.nl/files/146358924/Embedding_Adaptive_Features_in_the_ArduPilot_Control_Architecture_for_Unmanned_Aerial_Vehicles.pdf#:~:text=need

Adaptive PID Tuning Methods
Adaptive control methods adjust controller parameters on the fly to respond to changing system dynamics or operating conditions. Instead of using one static set of gains, an adaptive PID scheme continually or periodically updates the PID gains in real-time based on measured performance or known scheduling variables. Open-source UAV communities and researchers have been actively investigating adaptive approaches to improve cascaded PID loops without sacrificing the proven architecture. Two major categories of adaptive tuning are gain scheduling (pre-planned gain variations) and model reference adaptive control (continuous parameter adjustment to track a desired model). We discuss these below, along with other adaptive strategies, and how they can enhance UAV control.

Gain Scheduling
Gain scheduling is a straightforward adaptive strategy where the controller gains are changed as a function of some measured variable or operating condition. Essentially, the PID gains are “scheduled” on known parameters that correlate with dynamic changes – for example, airspeed, altitude, battery voltage, or vehicle weight. Unlike a fixed controller, a gain-scheduled PID has multiple sets of gains tailored to different regimes and smoothly transitions between them as conditions change. This approach has long been used in aircraft autopilots: e.g. fixed-wing UAVs often scale their gains with airspeed so that the effective control authority remains consistent ([Airspeed Parameters Setup — Plane documentation](https://ardupilot.org/plane/docs/airspeed-parameters-setup.html#:~:text=Since the effect of flying,no airspeed sensor is used)). In ArduPilot’s plane firmware, the PID loop gains are automatically scaled by the estimated airspeed to “prevent oscillations at higher airspeeds and preserve stabilization at low airspeeds” ([Airspeed Parameters Setup — Plane documentation](https://ardupilot.org/plane/docs/airspeed-parameters-setup.html#:~:text=Since

For VTOL UAVs, gain scheduling (or mode-dependent gains) is practically essential. During a VTOL’s hover-to-forward transition, the aircraft’s dynamics shift from helicopter-like to airplane-like. Developers can define different PID gain sets for hover versus cruise and interpolate between them during transition. A recent control design example showed a gain-scheduled PID autotuner for a tiltrotor VTOL that adjusts the controllers in-flight during transitions ([Gain-Scheduled PID Autotuning a VTOL UAV During Forward and Backward Transition - MATLAB & Simulink ](https://www.mathworks.com/help/slcontrol/ug/gain-scheduled-control-vtol-uav.html#:~:text=This example shows how to,UAV Toolbox)). By scheduling gains as a function of tilt angle or airspeed, the system maintained stability and performance throughout the difficult transition phase. Without scheduling, a single set of gains would either be too aggressive in one mode or too weak in the other.

Gain scheduling is attractive because it’s conceptually simple and leverages existing intuition about how dynamics change. The implementation overhead is modest – it requires measuring the scheduling variable (airspeed, weight, etc.) and either switching to a preset gain set or interpolating between gain values. Many open-source autopilots already support multiple sets of gains for different flight modes (for instance, separate hover and cruise PIDs in a VTOL configuration). The key is identifying the correct scheduling parameters and mapping. Drawbacks of gain scheduling include the need to correctly tune multiple sets of gains (one for each regime) and the assumption that the scheduling variable adequately captures the dynamic change. If the actual system change doesn’t correlate cleanly with the chosen variable, or if an unexpected condition occurs, gain scheduling alone may not fully compensate (it is essentially an open-loop adaptation). Still, when the operating regimes are known and can be measured, gain scheduling provides an effective, low-complexity way to broaden a PID controller’s operating range.

Model Reference Adaptive Control (MRAC)
Model Reference Adaptive Control is a more advanced adaptive strategy where the controller adjusts its parameters in real-time to make the plant’s behavior follow a desired reference model. In the context of a PID-controlled UAV, MRAC can be used to continually tweak the PID gains (or add additional adaptive terms) so that the UAV’s response matches a “reference model” that represents ideal response. Unlike gain scheduling, MRAC does not rely on pre-set gain tables; instead, it uses feedback laws to automatically tune parameters based on the difference between the actual UAV response and the reference model output.

In an MRAC system, one defines a reference model (for example, a first-order or second-order system that reflects the desired handling qualities for attitude). The UAV’s actual attitude response (under the current PID gains) is compared to this model’s response. Any deviation generates an error signal used by an adaptive law (often based on Lyapunov stability or MIT rule) to adjust controller parameters, minimizing the error over time ([A Modified Model Reference Adaptive Controller (M-MRAC) Using an Updated MIT-Rule for the Altitude of a UAV](https://www.mdpi.com/2079-9292/9/7/1104#:~:text=we can not predict the,be presented in this paper)). Essentially, the PID loop gains are not fixed – they are augmented by an adaptation mechanism that learns the correct gains to make the real UAV behave like the model. For instance, if a quadrotor suddenly becomes more sluggish due to added mass or a motor issue, the MRAC will sense that the attitude response is slower than the model’s and will increase the appropriate gains to compensate, all online.

Researchers have demonstrated MRAC-based enhancements for UAV PIDs. One study developed a modified MRAC altitude controller for a drone that had to catch a target, thereby undergoing a sudden mass increase ([A Modified Model Reference Adaptive Controller (M-MRAC) Using an Updated MIT-Rule for the Altitude of a UAV](https://www.mdpi.com/2079-9292/9/7/1104#:~:text=will increase significantly with the,MMRAC) for)) ([A Modified Model Reference Adaptive Controller (M-MRAC) Using an Updated MIT-Rule for the Altitude of a UAV](https://www.mdpi.com/2079-9292/9/7/1104#:~:text=we can not predict the,be presented in this paper)). The adaptive controller compared the drone’s altitude response to a desired model and adjusted the PID output to minimize the difference. This allowed the drone to maintain altitude control even after the drastic change in mass, outperforming the fixed-gain PID which struggled after impact. More generally, MRAC has been proposed for multirotor attitude control to handle uncertainties without requiring an explicit model update – the controller “learns” the new dynamics on the fly ([Design of Attitude Control System for UAV Based on Feedback ...](https://onlinelibrary.wiley.com/doi/10.1155/2014/492680#:~:text=Design of Attitude Control System,for a fixed wing UAV)) ([[PDF] Attitude Control of a Quadcopter Using Adaptive Control Technique](https://pdfs.semanticscholar.org/5f0a/5d9a511568192fd16ea39ce3560b70c7f4ca.pdf#:~:text=Technique pdfs,information of the plant)). In practice, implementing MRAC on resource-limited flight controllers is challenging but feasible: it requires additional computation for the adaptation law (e.g. integrating parameter update equations each time step) and careful tuning of adaptation rates to ensure stability (to avoid fast parameter swings or instability due to time delays). Some recent efforts have shown success embedding adaptive algorithms into ArduPilot’s architecture without altering its PID structure ([Embedding Adaptive Features in the ArduPilot Control Architecture for Unmanned Aerial Vehicles](https://research.tudelft.nl/files/146358924/Embedding_Adaptive_Features_in_the_ArduPilot_Control_Architecture_for_Unmanned_Aerial_Vehicles.pdf#:~:text=work presents a systematic integration,Autopilot)) ([Embedding Adaptive Features in the ArduPilot Control Architecture for Unmanned Aerial Vehicles](https://research.tudelft.nl/files/146358924/Embedding_Adaptive_Features_in_the_ArduPilot_Control_Architecture_for_Unmanned_Aerial_Vehicles.pdf#:~:text=such uncertainties and perturbations,in ArduPilot without modifying its)) – essentially bolting on an adaptation layer that continuously nudges the gains. Tests of an “adaptive ArduPilot” have shown improved performance in uncertain flight conditions compared to the stock fixed-gain controller ([Embedding Adaptive Features in the ArduPilot Control Architecture for Unmanned Aerial Vehicles](https://research.tudelft.nl/files/146358924/Embedding_Adaptive_Features_in_the_ArduPilot_Control_Architecture_for_Unmanned_Aerial_Vehicles.pdf#:~:text=altering the original architecture%2C to,source code of the proposed)).

MRAC’s advantage is its ability to handle unanticipated changes. Unlike gain scheduling, it does not need prior knowledge of the exact condition change; it will respond to any discrepancy between expected (modeled) behavior and actual behavior. This makes it powerful for “unstructured uncertainties” like unknown payloads or aerodynamic disturbances (Embedding Adaptive Features in the ArduPilot Control Architecture for Unmanned Aerial Vehicles). The trade-off is complexity – designing a stable adaptive law and verifying it won’t destabilize the aircraft is non-trivial. There’s also a risk that adaptation can be thrown off by sensor noise or rapid maneuvers (though techniques like forgetting factors or sigma-modification can mitigate parameter drift). Nonetheless, MRAC and related adaptive schemes (like L1 adaptive control, a modern variant that provides a bounded quick-response adaptation with guaranteed robustness) are promising paths to make UAV PIDs more robust and self-tuning in real time. Indeed, experimental implementations of L1 adaptive augmentation on quadrotors have shown it can maintain stability under severe model uncertainties, all while the base PID gains remain largely the same and the adaptive element compensates for the rest ([New control for the UAV - ArduPilot Discourse](https://discuss.ardupilot.org/t/new-control-for-the-uav/18691#:~:text=I'd go looking in github,done with L1 adaptive control)) ([Need help with adaptive controllers for copter - ArduCopter](https://discuss.ardupilot.org/t/need-help-with-adaptive-controllers-for-copter/68459#:~:text=Need help with adaptive controllers,adaptive control method for)).

Other Adaptive Techniques (Self-Tuning, Fuzzy Logic)
Beyond classical MRAC, there are other adaptive approaches worth mentioning. Self-tuning regulators can use online system identification to estimate the UAV’s current dynamics and then recompute PID gains on the fly (using formulas like pole placement or optimal control for the updated model). This involves intermittently exciting the system to identify parameters, which can be tricky during flight, but some autopilots have modes to auto-tune by applying test maneuvers ([PID Autotuning for UAV Quadcopter - MATLAB & Simulink ](https://la.mathworks.com/help/uav/ug/pid-autotuning-for-uav-quadcopter.html#:~:text=controllers%2C including manual tuning and,and are not easily repeatable)) ([PID Autotuning for UAV Quadcopter - MATLAB & Simulink ](https://la.mathworks.com/help/uav/ug/pid-autotuning-for-uav-quadcopter.html#:~:text=Run Autotuning Mission)). For example, the PX4 and ArduPilot projects have autotune modes that effectively perform system identification and PID tuning in a controlled manner during flight (though these are typically used offline – i.e. the UAV hovers and excites each axis to find good gains, rather than continuously updating them during normal flight). This kind of on-demand adaptation still improves usability, making it easier to get optimal PID values for a given vehicle.

Another technique uses fuzzy logic for gain scheduling. Fuzzy adaptive controllers can be seen as an expert system that adjusts PID gains based on linguistic rules and measured states, blending multiple operating points smoothly. A fuzzy logic controller can take inputs like “current altitude error” or “velocity” and infer appropriate gain adjustments in a continuous manner (rather than switching hard at preset points). One recent work proposed a “fuzzy gain-scheduling mechanism” to adjust a quadrotor’s position and altitude PID controllers, aiming for a strategy that is effective, simple, and robust to uncertainties and external disturbances ([Fuzzy Gain-Scheduling PID for UAV Position and Altitude Controllers](https://www.mdpi.com/1424-8220/22/6/2173#:~:text=The main contribution of this,can be summarized as follows)). The PID gains were tuned and varied according to fuzzy rules that considered environmental conditions, yielding better performance than a fixed PID in both simulations and real tests ([Fuzzy Gain-Scheduling PID for UAV Position and Altitude Controllers](https://www.mdpi.com/1424-8220/22/6/2173#:~:text=strategy based on a fuzzy,can be summarized as follows)) (Fuzzy Gain-Scheduling PID for UAV Position and Altitude Controllers). The benefit of fuzzy scheduling is that it can handle nonlinear mappings between conditions and gains and can be easier to design when precise modeling is difficult (you incorporate human tuning expertise into the rules). It’s computationally light (just rule evaluations) and can be implemented on microcontrollers. The challenge is designing the rule base and membership functions correctly; this often requires trial and error or expert knowledge, and like standard gain scheduling, it is only as good as the scenarios anticipated by the rules.

In summary, adaptive PID approaches – whether through scheduled gain profiles, continuous MRAC-type adjustment, or rule-based intelligent tuning – provide a path to maintain control performance across a range of flight conditions. They allow the UAV’s control system to track changing dynamics in real-time and reduce the need for constant manual re-tuning. These methods have been demonstrated to improve stability margins and disturbance rejection when the UAV’s behavior deviates from the original design model. However, they also introduce additional complexity and must be implemented carefully to ensure they do not themselves become a source of instability. We next explore how machine learning techniques can further automate or optimize the tuning process, potentially discovering adaptive strategies beyond those achievable with classical methods.

Machine Learning-Based Tuning Approaches
Machine learning (ML) techniques, including reinforcement learning and neural networks, are being applied to UAV control to push beyond the limits of human-tuned PID gains. The idea is that an ML algorithm can learn optimal or adaptive control policies through data or simulation, potentially handling complex nonlinear dynamics and diverse conditions that are hard to address with manual tuning or simple adaptive laws. In the context of cascaded PID controllers, ML can be used in a few ways: (1) to automatically tune the PID gains (either offline or gradually online), (2) to provide a learned mapping from state/condition to PID gains (a kind of learned gain scheduler), or (3) to replace or augment the PID loops with a learned controller (e.g. a neural network controller that functions alongside or in place of a PID). Here we focus on approaches that enhance the PID tuning, since completely replacing the low-level controllers in open-source UAVs is less common due to safety concerns.

Reinforcement Learning for Adaptive Gain Tuning
Reinforcement Learning (RL) is a powerful framework where an agent learns to make decisions (in this case, adjusting PID gains or direct control outputs) through trial-and-error interactions with the environment, aiming to maximize a reward (such as minimizing tracking error). For UAV attitude control, an RL agent can be trained (typically in simulation) to adjust the PID parameters in real-time based on the current flight condition or error dynamics. The goal is to have the RL agent learn a control policy that yields better performance than static gains without human intervention.

A recent study demonstrated the effectiveness of this approach by using the Proximal Policy Optimization (PPO) algorithm to train an agent that adapts the gains of a cascaded PID controller in-flight for a quadcopter ([Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter Control](https://arxiv.org/html/2403.07216v1#:~:text=The paper presents a technique,over 40 decrease in tracking)). The RL policy observes the state of the vehicle (for example, tracking error, perhaps rates or other signals) and outputs adjustments to the PID gains as it flies. Over many training episodes in simulation, the agent learns how to tweak the gains to minimize trajectory tracking error. The results were promising – the adaptive gain policy achieved over a 40% reduction in tracking error compared to the best static-gain PID tuned for that same task ([Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter Control](https://arxiv.org/html/2403.07216v1#:~:text=Policy Optimization ,to the static gain controller)). This highlights that an RL-trained policy can find nuanced, time-varying adjustments that outperform even a well-tuned fixed controller, especially in scenarios with varying conditions or maneuvers. Importantly, the learned policy in that study was able to generalize to different trajectories, indicating a degree of robustness.

In practice, one would train such an RL-based tuner in a high-fidelity simulator to avoid crashing real UAVs during the learning phase (since RL initially explores random actions that could be unsafe) ([Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter Control](https://arxiv.org/html/2403.07216v1#:~:text=As the dynamics of a,avoid computational complexity%2C the quadcopter)). Once trained, the resulting policy (often a neural network mapping from state to PID gains) can be deployed on the UAV. Executing the policy onboard is usually not too demanding computationally if the network is small, but training it required considerable simulation data. The advantage is that the UAV then carries a sort of “pilot” that can continuously retune itself. For example, if wind gusts start affecting the drone, the RL policy might increase certain gains to respond faster, then lower them when conditions calm – all learned from experience rather than explicitly programmed.

One must note the trade-offs and challenges: RL policies lack formal guarantees and can behave unexpectedly outside their training distribution. They are only as good as the scenarios they were trained on – an RL-tuned controller might still falter if a completely novel disturbance occurs that it hasn’t experienced. There are also computational considerations: while running a small neural network at, say, 100 Hz is feasible on modern flight controllers (especially with ARM Cortex M7/H7 class MCUs or onboard companion computers), the complexity is higher than a simple PID update. Additionally, safety considerations often dictate that such learning-based controllers be used as an add-on or in a supervisory capacity rather than the sole control method, at least until they are proven. Nevertheless, RL provides a way to automatically discover adaptive control strategies that humans might not easily derive. It’s an active research area, with experiments in using deep RL to tune PID gains, or even to directly control UAV attitude via neural network policies. The 40% performance improvement result ([Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter Control](https://arxiv.org/html/2403.07216v1#:~:text=Policy Optimization ,to the static gain controller)) suggests that with careful design, an RL-tuned cascaded PID could significantly enhance tracking and disturbance rejection.

Neural Networks and Evolutionary Tuning
Apart from reinforcement learning, other machine learning approaches have been applied to PID tuning. Neural networks can serve as function approximators that output PID gains or even control signals based on the current state. For instance, a neural network could be trained (through supervised learning) to imitate an expert’s tuning or to map operating conditions to appropriate controller gains. Some early works used neural networks with actor-critic structures to tune PID gains, showing improved performance over classical tuning methods ([Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter Control](https://arxiv.org/html/2403.07216v1#:~:text=Learning based techniques have been,15]. Shuprajhaa et)). A neural network can also learn to compensate for model uncertainties: one approach is to have a neural adaptive element in parallel with the PID, learning the difference between the expected and actual dynamics (this veers into the territory of neuro-adaptive control).

Moreover, evolutionary algorithms and other optimization techniques (like genetic algorithms, particle swarm optimization, or Bayesian optimization) have been used to find PID parameters that optimize performance criteria. While these are typically used offline (e.g. optimizing gains in simulation for various conditions), one could envision an onboard implementation that periodically re-optimizes gains using quick experiments, though that is less common. The open-source community sometimes uses such optimization offline to derive better default PID sets for certain frames or aircraft.

A practical example of ML-assisted tuning in open source is ArduPilot’s autotune feature. While not “learning” in the AI sense, it uses a systematic search (which can be likened to an optimization algorithm) to adjust gains. The user activates autotune and the multicopter performs a sequence of twitches in each axis, measuring response, and incrementally adjusting PIDs to converge toward a desired damping and responsiveness. This can be seen as automated system identification plus tuning – an algorithmic approach to find gains without user trial-and-error. The result is a reasonably tuned PID for that vehicle in calm conditions. Machine learning could extend this by adjusting those gains as conditions change, not just once.

Fuzzy logic controllers mentioned earlier can also be considered here, since they often fall under intelligent control. A fuzzy PID essentially uses a rule-based “intelligence” to modify gains or control actions, and can even be tuned via learning algorithms (e.g. using genetic algorithms to evolve the fuzzy rule base). The result is a controller that can outperform linear PIDs under nonlinear conditions by applying different rules in different situations – effectively a human-designed adaptive controller.

In terms of real-world usage on resource-constrained systems: deploying a pre-trained neural network (from RL or supervised learning) is quite possible on modern autopilot hardware. Many microcontrollers can run basic neural nets (there are libraries for ARM Cortex-M that run inferencing). The key is to keep the model small (a few dozen neurons) to meet real-time requirements and memory limits. The computational cost of a single forward-pass of a small network at 200 Hz is not prohibitive with proper optimization. So, an RL-tuned gain scheduler could be embedded as a lightweight function. The training, however, is done off-board in powerful computers or cloud resources.

In summary, machine learning provides tools to automate and generalize PID tuning, potentially yielding better performance across complex scenarios. RL can create policies that adjust gains in real-time in response to the drone’s state, and neural networks/fuzzy logic can encode adaptive control rules. The trade-off is that these methods are complex to design and verify. Robustness is a concern – unlike classical control laws, ML controllers may not have guaranteed stability margins. Therefore, a common approach is a hybrid: use ML to enhance the PID (by recommending gain changes or augmenting control outputs), while keeping the fundamental cascaded PID structure as a safety net. This way, even if the ML part behaves unexpectedly, the underlying PID prevents catastrophic failure. As computational capabilities on UAVs grow (and with technologies like flight control on companion computers), we expect to see more ML-based tuning and adaptive control making it into open-source projects in the coming years.

Sensor Fusion and Enhanced Feedback for Control
Another critical aspect of improving cascaded PID performance is not just adjusting the controller, but improving the feedback signals that the controller uses. PID control is only as good as the measurement of the state (attitude, angular rates, etc.) – noisy or biased feedback can force the PID to compensate incorrectly. Thus, advanced sensor fusion techniques are employed in UAV flight controllers to provide high-quality state estimates, which in turn significantly enhance control performance and robustness.

All major open-source UAV platforms use sensor fusion (e.g. complementary filters or Extended Kalman Filters) to estimate the drone’s attitude, altitude, velocity, and position from raw sensor data. For example, ArduPilot and PX4 run an EKF (Extended Kalman Filter) that fuses data from gyroscopes, accelerometers, magnetometers, GPS, barometers, and other sensors to produce a coherent estimate of the vehicle’s orientation and position ([Extended Kalman Filter (EKF) — Copter documentation](https://ardupilot.org/copter/docs/common-apm-navigation-extended-kalman-filter-overview.html#:~:text=An Extended Kalman Filter ,airspeed and barometric pressure measurements)). By fusing multiple sensors, the EKF can reject faulty or noisy measurements and fill in gaps (e.g. using IMU data to propagate state when GPS is momentarily unavailable) ([Extended Kalman Filter (EKF) — Copter documentation](https://ardupilot.org/copter/docs/common-apm-navigation-extended-kalman-filter-overview.html#:~:text=The advantage of the EKF,be used to assist navigation)). This results in much more reliable attitude feedback than using a single sensor alone. The advantage is clear: the PID loops operating on these estimates will perform better – they won’t be misled by, say, a momentary GPS glitch or a magnetometer disturbance. In ArduPilot’s documentation, it’s noted that the EKF makes the vehicle “less susceptible to faults that affect a single sensor” and enables use of additional sensors like optical flow for better navigation ([Extended Kalman Filter (EKF) — Copter documentation](https://ardupilot.org/copter/docs/common-apm-navigation-extended-kalman-filter-overview.html#:~:text=The

For attitude control, sensor fusion means combining gyroscope and accelerometer (and often magnetometer) data to get an accurate and drift-free attitude. A simple complementary filter or a digital attitude algorithm (like DCM) can do this at low computational cost, but an EKF is more robust. With accurate attitude estimates, the outer-loop PID that holds a level orientation can work with minimal error; with accurate angular velocity (gyro) estimates filtered of noise, the inner rate PID can use higher gains without amplifying noise (since a filtered derivative term is used) ([Controller Diagrams | PX4 Guide (main)](https://docs.px4.io/main/en/flight_stack/controller_diagrams.html#:~:text=,filtered derivative to the controller)). Many flight controllers also use notch filters and low-pass filters on gyro data (often informed by motor RPM sensing) to eliminate vibration-induced noise. This is a form of sensor-data-driven adaptation: for example, the “dynamic notch filter” in ArduPilot and Betaflight monitors motor RPM or IMU spectrum to automatically target and remove frame resonance frequencies. By doing so, it cleans up the feedback signal so the PID doesn’t react to noise. The result is the ability to increase P gains for tighter control without causing oscillations from noise. While not “sensor fusion” in the sense of combining different sensor modalities, it is an adaptive signal processing technique that significantly improves control quality.

Another aspect of sensor fusion is using additional sensing to augment control. For instance, a fixed-wing UAV with a wind vanes or sideslip sensors can detect gusts and feed that into a controller to counteract turbulence faster than waiting for an error to develop. Or a multicopter with a visual inertial odometry system (camera + IMU) can hold position in GPS-denied environments, providing position feedback that a simple GPS-based system couldn’t. These additional fused inputs don’t change the PID algorithm itself, but they improve the information fed to it, effectively allowing the existing controller to perform at a higher level of precision. Sensor fusion can also provide estimates of disturbances or model parameters (e.g. estimating wind velocity affecting a fixed-wing, or estimating the mass of a slung load in-flight). A controller could use those estimates to adjust feed-forward terms or gains – for example, if the wind estimator senses a strong constant wind, the controller could add a feed-forward to the aileron to counteract it, reducing steady-state error burden on the PID.

In summary, robust sensor fusion is a crucial enabler of high-performance control in UAVs. By delivering accurate, high-bandwidth state estimates (and sometimes even higher-order derivatives or disturbance estimates), it allows PID controllers to operate with minimal uncertainty about the true state. This means fewer false error signals, less noise-induced actuation, and better handling of external disturbances. While sensor fusion itself does not adapt the PID gains, it adapts the quality of the feedback in real-time by weighting and selecting sensor data. The cascaded PID loops in an open-source flight controller like PX4 explicitly depend on the EKF2 estimates for their feedback ([Controller Diagrams | PX4 Guide (main)](https://docs.px4.io/main/en/flight_stack/controller_diagrams.html#:~:text=,shown as a)), underlining that without good sensor fusion, even the best-tuned PID can underperform or become unstable. The computational cost of these algorithms (EKF running at 100-400 Hz, plus filtering) is moderate but has been optimized for embedded use in projects like ArduPilot (using multiple IMU cores and filtering). The benefit to robustness is well worth the CPU cycles: for example, the EKF can detect and ignore a “stuck” barometer or a GPS jump, preventing the controller from reacting violently to a bad reading – something a simple PID using raw sensor data could not do. Thus, improving control performance is not only about tuning gains, but also about ensuring the controller is measuring what it thinks it is. As UAVs incorporate more advanced sensors (depth cameras, LiDAR, etc.), fusing these into the control loop can further enhance stability (e.g. terrain following and holding altitude precisely using LiDAR inputs, which reduces oscillation compared to barometer alone).

Real-Time PID Parameter Adjustment
The ultimate goal of many of the methods discussed (adaptive control, RL, etc.) is to achieve real-time PID parameter adjustment – the ability for the UAV’s controller to autonomously modify its behavior on the fly to maintain optimal performance. Real-time adjustment can be done explicitly (changing the PID gains or adding terms in real time) or implicitly (via an adaptive element that changes the control output). We have touched on how each method accomplishes this:

Gain scheduling provides piecewise-optimal gains by instantly switching or interpolating based on a real-time measured condition. It’s real-time in the sense that as soon as, say, airspeed changes, the gains used by the control loop at that instant are updated according to a schedule. This happens continuously during flight but is predetermined by a schedule map.
Adaptive controllers (MRAC, L1) perform continuous parameter estimation and update during flight. The PID gains or an auxiliary adaptive control signal are continually adjusted every control cycle based on the current tracking error and adaptation law. This is true closed-loop real-time adaptation: the controller is effectively “learning” the system’s current behavior every moment and altering its parameters accordingly.
Reinforcement learning-based policies typically don’t “learn” during the flight (learning is done offline), but the deployed policy will adjust the effective gains or control outputs in real time as a function of the state. For instance, an RL gain-scheduler network might output higher D gain when it senses rapid error growth or reduce P gain when overshoot is detected, etc., doing so at each time step. So it behaves like an adaptive controller whose rulebook was learned via RL. There is also research into online RL where the system keeps learning during deployment, but in safety-critical UAV control this is rarely used due to unpredictability.
Fuzzy logic controllers adjust outputs/gains in real time by evaluating rules on the current state. If the UAV ascends and the altitude error is high, a fuzzy rule might temporarily boost the altitude PID P gain for quicker response; when nearing the target, another rule might reduce it to avoid overshoot. All this happens instantaneously based on sensor readings.
Sensor fusion and filters adjust the feedback in real time (e.g., an EKF constantly recalibrates the state estimate as sensor readings come in). Some sensor-related adjustments also happen, like auto-adjusting filter cut-off frequencies with motor speed or temperature compensation for barometers – these indirectly help maintain PID effectiveness in real time by keeping sensor data consistent.
A critical consideration for real-time adjustment algorithms is computational cost and latency. UAV control loops are fast (typically 100 Hz for outer loops and 200–1000 Hz for inner loops on multirotors). Any adaptive algorithm must run within these loops without introducing significant delay. Simpler approaches like gain scheduling or fuzzy logic are essentially just table lookups or a few rule evaluations – negligible overhead. MRAC involves additional state variables (the adapted parameters) and differential equations for their update, which can be computationally light if there are only a few parameters, but heavy if adapting a lot of gains. In practice, adapting 2–3 gains per axis is manageable; adapting a full state-space matrix would be more intensive. Reinforcement learning controllers running a neural network must ensure the network is small enough to compute within the loop cycle. Modern flight controller hardware (e.g. an STM32H7 at 400 MHz) can handle a few hundred multiplications per loop easily, so a small NN (say 3 layers of 16 neurons) is feasible at 1 kHz, especially if using fixed-point optimizations.

Another factor is stability and safety: real-time gain changes can potentially destabilize a system if done abruptly or erroneously. Hence many adaptive schemes include safeguards. For example, gain scheduling might be implemented with interpolation to avoid sudden jumps, or with limits to prevent gains going beyond safe bounds. MRAC often includes normalization or projection to keep estimated gains within reasonable ranges. RL policies can be constrained or combined with a conventional PID (e.g., blending the outputs) to ensure the output stays in a safe envelope. It’s also common to fallback to manual gains if the adaptive element fails or triggers a fault detection.

In terms of optimal selection of PID parameters, researchers are also exploring auto-tuning in real-time. One concept is iterative learning control where each repetition of a maneuver (say each waypoint navigation) slightly updates controller parameters to reduce error in the next iteration. Over multiple iterations, the controller “converges” to an optimal setting for that maneuver. This isn’t instantaneous adjustment, but rather adjustment over repeated tasks.

Overall, achieving reliable real-time adjustment of PID parameters is a step towards truly self-driving, self-calibrating UAVs that can adapt to damage, payload changes, or environmental extremes without human intervention. The methods we discussed vary in maturity and complexity, but all serve this goal. Next, we compare these methods along key dimensions to understand their trade-offs in the context of UAV attitude control on resource-constrained hardware.

Trade-offs and Comparison of Enhancement Methods
Improving a UAV’s control performance with the above techniques involves balancing trade-offs in complexity, computational load, robustness, and ease of use. We provide a comparative overview of the discussed methods:

Fixed PID (Baseline): Computational Cost: Minimal – very efficient to run. Robustness: Limited to conditions near the tuning point; performance degrades outside that range ([Embedding Adaptive Features in the ArduPilot Control Architecture for Unmanned Aerial Vehicles](https://research.tudelft.nl/files/146358924/Embedding_Adaptive_Features_in_the_ArduPilot_Control_Architecture_for_Unmanned_Aerial_Vehicles.pdf#:~:text=parameters alter the flight dynamics,controllers to avoid loss of)). Ease of Implementation: Easiest – well-known, supported by all autopilots; the main burden is manual tuning for each vehicle and condition. Memory/CPU Footprint: Negligible. Notes: Fails gracefully (tends to be stable but with poor performance) if conditions change slightly, but can become unstable in drastic condition changes (e.g. large payload) without re-tune ([Embedding Adaptive Features in the ArduPilot Control Architecture for Unmanned Aerial Vehicles](https://research.tudelft.nl/files/146358924/Embedding_Adaptive_Features_in_the_ArduPilot_Control_Architecture_for_Unmanned_Aerial_Vehicles.pdf#:~:text=need to minimize the alteration,controllers to avoid loss of)).
Gain Scheduling: Computational Cost: Low – just interpolating or switching gains based on sensor input. Robustness: Good within the predefined regimes – handles predictable changes (like speed, flight phase) very well ([Airspeed Parameters Setup — Plane documentation](https://ardupilot.org/plane/docs/airspeed-parameters-setup.html#:~:text=Since the effect of flying,no airspeed sensor is used)). Not robust to unforeseen changes outside scheduled parameters. Ease of Implementation: Moderate – requires identifying key scheduling variables and tuning multiple gain sets. Many autopilots support it (e.g. airspeed scaling in ArduPlane) ([Airspeed Parameters Setup — Plane documentation](https://ardupilot.org/plane/docs/airspeed-parameters-setup.html#:~:text=Since
MRAC / Adaptive PID: Computational Cost: Moderate – continuous parameter adaptation calculations, but feasible on modern MCUs for a few gains. Robustness: Potentially high – can handle unexpected disturbances and model changes by adjusting on the fly (Embedding Adaptive Features in the ArduPilot Control Architecture for Unmanned Aerial Vehicles) ([Embedding Adaptive Features in the ArduPilot Control Architecture for Unmanned Aerial Vehicles](https://research.tudelft.nl/files/146358924/Embedding_Adaptive_Features_in_the_ArduPilot_Control_Architecture_for_Unmanned_Aerial_Vehicles.pdf#:~:text=altering
L1 Adaptive Control: (a specific adaptive method, subset of MRAC) Computational Cost: Moderate, similar to MRAC. Robustness: Very high within designed bounds – L1 is known for maintaining stability even with fast adaptation, by decoupling the adaptation from the feedback loop. Ease: Still complex to implement and tune, though conceptually a bit more plug-and-play than general MRAC. Maturity: Research to early adoption – e.g. experiments showing L1 can augment a PID for agile flight ([New control for the UAV - ArduPilot Discourse](https://discuss.ardupilot.org/t/new-control-for-the-uav/18691#:~:text=I'd go looking in github,done with L1 adaptive control)). Not in default autopilots yet, but likely to appear as an option.
Reinforcement Learning-Based Tuning: Computational Cost: Low to run (inference of a small neural network or policy logic), but very high to train (which is done offboard). Robustness: Can be very good within the training distribution – e.g. significantly lower tracking error ([Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter Control](https://arxiv.org/html/2403.07216v1#:~:text=Policy Optimization ,to the static gain controller)) – but uncertain outside it. No theoretical stability guarantees, relies on exhaustive training. Ease of Implementation: Very high complexity – requires setting up simulation environments, reward design, training infrastructure. Not something an average user would do; more likely comes in form of pretrained solutions. Onboard Requirements: If using a neural net, need enough CPU for matrix ops; typically manageable. Maturity: Research phase – promising results in simulation ([Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter Control](https://arxiv.org/html/2403.07216v1#:~:text=Policy
Fuzzy Logic / Expert System Tuning: Computational Cost: Low – rule evaluations at runtime are quick. Robustness: Good if rules cover the scenario – demonstrated to handle uncertainties and disturbances by adjusting gains accordingly ([Fuzzy Gain-Scheduling PID for UAV Position and Altitude Controllers](https://www.mdpi.com/1424-8220/22/6/2173#:~:text=The main contribution of this,can be summarized as follows)). However, like scheduling, it won’t handle totally unanticipated conditions unless the rules somehow accommodate them. Ease: Moderate – requires designing the fuzzy rules and membership functions (can be time-consuming, but tools exist). Implementation: Can be added onto a system relatively easily; can even run on a companion computer if the flight CPU is tight. Maturity: Fuzzy controllers have been around for decades; not mainstream in open-source UAVs, but some custom projects use them. They offer a middle ground between hard-coded scheduling and black-box learning.
Improved Sensor Fusion: Computational Cost: Moderate – e.g. EKF takes a fraction of CPU time but is well optimized in autopilots ([Extended Kalman Filter (EKF) — Copter documentation](https://ardupilot.org/copter/docs/common-apm-navigation-extended-kalman-filter-overview.html#:~:text=Current stable versions of ArduPilot,consistency of its sensor data)). Robustness: Very high – significantly reduces susceptibility to sensor errors and noise ([Extended Kalman Filter (EKF) — Copter documentation](https://ardupilot.org/copter/docs/common-apm-navigation-extended-kalman-filter-overview.html#:~:text=The advantage of the EKF,be used to assist navigation)), thus indirectly improving control robustness. It doesn’t change gains, but prevents many issues that would challenge the controller (like bad data or drift). Ease: High complexity to develop, but fortunately, it’s already built into most autopilot software. Tuning sensor fusion (like covariances) is occasionally needed but generally transparent to users. Benefit: As a supporting technique, it complements any of the above methods; essentially required for good performance. Maturity: Very mature – used in all modern UAVs (DCM or EKF); new sensor fusion improvements (e.g. visual-inertial odometry) are actively being integrated as optional modules.
The table below summarizes these methods along key dimensions:

Method	Adaptive Mechanism	Computational Cost	Robustness to Changes	Implementation Complexity
Fixed PID (Baseline)	None (static gains)	Very Low	Limited – tuned for nominal case; poor if dynamics change significantly ([Embedding Adaptive Features in the ArduPilot Control Architecture for Unmanned Aerial Vehicles](https://research.tudelft.nl/files/146358924/Embedding_Adaptive_Features_in_the_ArduPilot_Control_Architecture_for_Unmanned_Aerial_Vehicles.pdf#:~:text=parameters alter the flight dynamics,controllers to avoid loss of))	Very easy – standard in all controllers
Gain Scheduling	Predefined gain lookup/curve vs. measured condition (open-loop adaptation)	Very Low	Good for anticipated regime changes (e.g. speed) ([Airspeed Parameters Setup — Plane documentation](https://ardupilot.org/plane/docs/airspeed-parameters-setup.html#:~:text=Since the effect of flying,no airspeed sensor is used)); not effective for unmeasured or unexpected changes	Moderate – need multiple gain sets and scheduling logic (supported in fixed-wing autopilots)
MRAC / Adaptive PID	Continuous closed-loop gain adaptation to follow a reference model (closed-loop adaptation)	Moderate	High – adjusts gains to handle unknown model variations (Embedding Adaptive Features in the ArduPilot Control Architecture for Unmanned Aerial Vehicles) ([Embedding Adaptive Features in the ArduPilot Control Architecture for Unmanned Aerial Vehicles](https://research.tudelft.nl/files/146358924/Embedding_Adaptive_Features_in_the_ArduPilot_Control_Architecture_for_Unmanned_Aerial_Vehicles.pdf#:~:text=altering	
L1 Adaptive Augmentation	Fast adaptive loop added to PID (robust adaptive control)	Moderate	High – strong robustness to modeling errors while maintaining stability (theoretically guaranteed within bounds)	High – still research-level, custom integration needed
RL-Based Tuning	Learned policy adjusts gains or replaces controller (policy from offline training)	Low (runtime)High (training)	Potentially very high performance ([Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter Control](https://arxiv.org/html/2403.07216v1#:~:text=Policy Optimization ,to the static gain controller)) within trained scenarios; unpredictable outside them (no guarantees)	Very High – requires ML expertise and simulation training; not mainstream yet
Fuzzy Logic Controller	Rule-based gain adjustment (expert system)	Low	Good – handles nonlinearities and uncertainties per design of rules ([Fuzzy Gain-Scheduling PID for UAV Position and Altitude Controllers](https://www.mdpi.com/1424-8220/22/6/2173#:~:text=The main contribution of this,can be summarized as follows)); only as robust as rule coverage	Moderate – needs expert rules; simpler than full adaptive control design, but iterative to refine
Sensor Fusion (EKF, etc.)	Fuses sensor inputs for accurate state feedback (adaptive filtering)	Moderate	High – greatly improves stability and accuracy of control by avoiding sensor-induced errors ([Extended Kalman Filter (EKF) — Copter documentation](https://ardupilot.org/copter/docs/common-apm-navigation-extended-kalman-filter-overview.html#:~:text=The advantage of the EKF,be used to assist navigation)); still requires a well-tuned controller	High – complex algorithms, but provided by autopilot libraries; essential for modern UAVs
Table: Comparison of methods to enhance cascaded PID control in UAVs. Each method’s mechanism for improving control is listed, along with rough costs and benefits. Note that these methods are not mutually exclusive – e.g. an UAV can use EKF-based sensor fusion and gain scheduling, or an adaptive controller with sensor fusion, etc., to combine benefits.

Conclusion
Enhancing the performance of cascaded PID controllers in open-source UAV flight controllers is a multi-faceted challenge. Fixed-gain PID loops, while simple and reliable, cannot by themselves guarantee optimal attitude control across the full spectrum of flight conditions and UAV configurations. We have surveyed a range of practical solutions that extend or augment the PID framework to make it more adaptable and robust. Adaptive tuning methods such as gain scheduling provide a proven, low-complexity way to tailor PID gains to different flight regimes (widely used in fixed-wing and VTOL systems), whereas more advanced closed-loop adaptive schemes like MRAC and L1 adaptive control offer the promise of continuous self-adjustment to handle unforeseen dynamics (at the cost of greater design complexity). Machine learning approaches, especially reinforcement learning, are emerging as tools to automatically discover high-performance control policies that can adjust PID parameters in real time – these have demonstrated significant improvements in simulation, though real-world deployment is still in early stages. Complementary to tuning algorithms, sensor fusion techniques ensure the controller is always fed with high-quality state information, thereby improving stability and response without changing the control law itself. In practice, the best results may come from a hybrid approach: for example, using an EKF for state estimation, gain scheduling for known condition variations, and an adaptive element or learned policy to handle remaining uncertainties.

Each method carries trade-offs in computational load, robustness, and ease of implementation. Simpler methods (scheduling, filtering) are already within reach of today’s autopilot hardware and are implemented in platforms like ArduPilot and PX4. More complex methods (adaptive control, RL) are being actively researched and gradually integrated as optional enhancements; their adoption will depend on demonstrating reliability and safety in the field. On resource-constrained UAV systems, any new control algorithm must prove its worth relative to its CPU and memory footprint – fortunately, many adaptive algorithms have been streamlined, and increased processing power on newer autopilot boards is opening doors for sophisticated control logic (including possibly neural network inference).

In summary, cascaded PID controllers can be made far more capable through these enhancement techniques. A quadcopter or fixed-wing UAV that once needed constant PID retuning for each payload or weather change could, in the near future, auto-tune itself in flight, maintain stability through aggressive maneuvers via adaptive gains, and leverage AI to push performance limits while keeping the trusted PID structure as a backbone. The ongoing evolution of open-source UAV controllers is toward more autonomy and resilience: maintaining tight, reliable control in diverse scenarios with minimal human intervention. The combination of adaptive control theory and machine learning, supported by robust sensor fusion, is paving the way for UAVs that are not only easy to fly out-of-the-box but also intelligently adjust to whatever the flight throws at them, ensuring optimal attitude control at all times. The continued research and testing in this area will determine the optimal mix of methods for different UAV platforms, balancing performance gains against the practical constraints of onboard computation and the need for certifiable reliability. With careful integration, even resource-limited drones can enjoy significant improvements in control performance, making UAV flight safer and more efficient across an ever-expanding range of applications. ([A Modified Model Reference Adaptive Controller (M-MRAC) Using an Updated MIT-Rule for the Altitude of a UAV](https://www.mdpi.com/2079-9292/9/7/1104#:~:text=will increase significantly with the,MMRAC) for)) ([Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter Control](https://arxiv.org/html/2403.07216v1#:~:text=The paper presents a technique,over 40 decrease in tracking))