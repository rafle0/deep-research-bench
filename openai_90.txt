Liability in ADAS-Equipped Vehicle Accidents (SAE Levels 2 and 3)
Introduction
Advanced Driver-Assistance Systems (ADAS) are increasingly common, promising safety benefits by reducing human error (a factor in roughly 88% of collisions ([UK law makes OEMs responsible for self-driving crashes | Traffic Technology Today](https://www.traffictechnologytoday.com/news/autonomous-vehicles/uk-law-makes-oems-responsible-for-self-driving-crashes.html#:~:text=The potential safety benefits are,per cent of road collisions))). However, when vehicles with ADAS (particularly SAE Level 2 and Level 3 automation) are involved in crashes, determining liability becomes complex. In these “shared control” scenarios, responsibility is split between a human driver and the automated system. This report analyzes how technical limitations of ADAS contribute to accidents, and how current laws in the U.S., EU, and UK allocate liability between drivers and manufacturers. It reviews influential case law defining the boundaries of responsibility, examines common accident types (rear-end crashes, lane departures, intersection collisions) under existing liability interpretations, and considers differences for private drivers versus commercial fleets. The report concludes with recommendations for regulatory guidelines to address gaps and ambiguities in the current legal treatment of these incidents.

ADAS Technologies: Capabilities and Limitations
Modern Level 2 ADAS features automate certain driving tasks (steering, braking, acceleration) under human supervision. Key technologies include adaptive cruise control, lane-keeping assist, and automatic emergency braking. To understand liability issues, it is crucial to first explain what these systems can and cannot do.

Adaptive Cruise Control (ACC): ACC maintains a set speed like traditional cruise control, but also adjusts speed to keep a safe following distance behind a vehicle ahead ([ADAS and Car Accident Liability: What You Need to Know](https://stokesstemle.com/car-accident-lawyer/adas-and-car-accident-liability-what-you-need-to-know/#:~:text=,avoid drifting out of their)) ([ADAS and Car Accident Liability: What You Need to Know](https://stokesstemle.com/car-accident-lawyer/adas-and-car-accident-liability-what-you-need-to-know/#:~:text=the
Lane Keeping Assist (LKA) and Lane Departure Prevention: Lane-keeping systems use cameras to monitor lane markings and provide steering input or warnings to prevent unintentional lane drift ([ADAS and Car Accident Liability: What You Need to Know](https://stokesstemle.com/car-accident-lawyer/adas-and-car-accident-liability-what-you-need-to-know/#:~:text=,on collisions)) ([ADAS and Car Accident Liability: What You Need to Know](https://stokesstemle.com/car-accident-lawyer/adas-and-car-accident-liability-what-you-need-to-know/#:~:text=traffic
Automatic Emergency Braking (AEB): AEB uses forward-facing sensors (often paired with forward-collision warning) to detect imminent collisions and autonomously apply the brakes if the driver fails to react in time ([ADAS and Car Accident Liability: What You Need to Know](https://stokesstemle.com/car-accident-lawyer/adas-and-car-accident-liability-what-you-need-to-know/#:~:text=,systems use sensors along a)) ([ADAS and Car Accident Liability: What You Need to Know](https://stokesstemle.com/car-accident-lawyer/adas-and-car-accident-liability-what-you-need-to-know/#:~:text=typically
In summary, ADAS Level 2 technologies provide assistance – they do not replace an attentive human driver. Automakers explicitly warn that drivers must supervise these features at all times and be prepared to take over immediate control ([The Verdict Is In: California Jury Finds That Tesla’s “Autopilot” Feature Not Responsible For Fatal Accident | Knowledge | Fasken](https://www.fasken.com/en/knowledge/2023/11/california-jury-finds-that-teslas-autopilot-feature-not-responsible-for-fatal-accident#:~:text=suite of advanced driver,take over at any time)) ([Level 3 Automated Vehicles and Criminal Law - JURIST - Commentary - Legal News & Commentary](https://www.jurist.org/commentary/2023/08/widen-koopman-automated-vehicles-criminal-law/#:~:text=on the manufacturer’s design intent,as in the case of)). The technical limitations of sensors and software mean ADAS may not handle every situation. This reality sets the stage for liability questions: if a partially automated car crashes because the system didn’t perform as a human anticipated, should the blame lie with the human driver (for not intervening) or the manufacturer (for a system failure or design flaw)? We turn next to how different legal systems are currently tackling this question.

Legal Frameworks: Liability for ADAS-Related Crashes in Key Jurisdictions
United States
In the U.S., there is no single unified law specifically governing liability for Level 2–3 automated driving; instead, a patchwork of existing traffic laws, product liability doctrines, and emerging state statutes applies. Crucially, a Level 2 or 3 vehicle is not considered “fully self-driving” under U.S. law – the human operator is still deemed the driver and bears primary responsibility for safe operation. The National Highway Traffic Safety Administration (NHTSA) and SAE classify Level 2 systems as “driver support features” requiring full driver engagement ([Level 3 Automated Vehicles and Criminal Law - JURIST - Commentary - Legal News & Commentary](https://www.jurist.org/commentary/2023/08/widen-koopman-automated-vehicles-criminal-law/#:~:text=on the manufacturer’s design intent,as in the case of)). Therefore, if a Level 2 ADAS-equipped car is involved in an accident, the default legal position is that the human driver is liable under the normal rules of the road (just as if they were driving a conventional car). Traffic laws in every state require drivers to maintain control of their vehicle; using ADAS features does not excuse violations (speeding, running a red light, careless driving, etc.). In fact, prosecutors have pursued criminal charges against drivers in ADAS-related fatal crashes on the theory that ultimate responsibility still lies with the human operator ([Level 3 Automated Vehicles and Criminal Law - JURIST - Commentary - Legal News & Commentary](https://www.jurist.org/commentary/2023/08/widen-koopman-automated-vehicles-criminal-law/#:~:text=In both cases%2C automation controlled,in sentencing without jail time)). For example, after a 2019 crash in California in which a Tesla on Autopilot ran a red light and killed two people, the Tesla’s driver was charged with vehicular manslaughter – marking the first U.S. felony prosecution of a driver-assistance user ([Level 3 Automated Vehicles and Criminal Law - JURIST - Commentary - Legal News & Commentary](https://www.jurist.org/commentary/2023/08/widen-koopman-automated-vehicles-criminal-law/#:~:text=Current events focus a spotlight,under driver supervision%2C was engaged)) ([Level 3 Automated Vehicles and Criminal Law - JURIST - Commentary - Legal News & Commentary](https://www.jurist.org/commentary/2023/08/widen-koopman-automated-vehicles-criminal-law/#:~:text=In

That said, U.S. law also provides avenues for holding manufacturers accountable civilly when a vehicle system is defective or causes an accident. Product liability is the primary legal framework to evaluate manufacturer responsibility. Injured parties (which could include the driver themselves, passengers, or third parties) may bring claims alleging that an ADAS feature had a design defect, manufacturing defect, or inadequate warnings, and that this defect caused the crash. For instance, if automatic emergency braking failed to engage due to a flaw in the software, or if a driver reasonably relied on a lane-keeping aid that turned out to malfunction, the manufacturer could be sued for the resulting damages ([ADAS and Car Accident Liability: What You Need to Know](https://stokesstemle.com/car-accident-lawyer/adas-and-car-accident-liability-what-you-need-to-know/#:~:text=,in the ADAS’s computer code)) ([ADAS and Car Accident Liability: What You Need to Know](https://stokesstemle.com/car-accident-lawyer/adas-and-car-accident-liability-what-you-need-to-know/#:~:text=vehicle

Several high-profile U.S. investigations highlight this balance. The National Transportation Safety Board (NTSB), examining a string of Tesla ADAS crashes, concluded that overreliance on Autopilot and its “operational limitations” were contributing factors in some crashes ([NTSB: 'Operational Limitations' Played Major Rule in Tesla Autopilot Crash - Business Insider](https://www.businessinsider.com/r-ntsb-operational-limitations-played-major-rule-in-tesla-autopilot-crash-2017-9#:~:text=WASHINGTON (Reuters) ,autonomous "Autopilot" system)). In a 2016 fatal Florida accident, the Tesla’s Autopilot did not detect a crossing truck trailer (a design limitation), and the driver, who was not attentive, failed to brake – a combination of factors that led NTSB to fault both the driver and the system’s design ([NTSB: 'Operational Limitations' Played Major Rule in Tesla Autopilot Crash - Business Insider](https://www.businessinsider.com/r-ntsb-operational-limitations-played-major-rule-in-tesla-autopilot-crash-2017-9#:~:text=WASHINGTON

European Union
Across the EU, the legal landscape is also evolving, but a common theme is that the human driver (or vehicle owner/keeper) remains liable by default for accidents, even if ADAS was in use. Most EU jurisdictions impose a form of strict liability on vehicle owners for traffic accidents. For example, many countries follow the principle (embodied in the European Motor Insurance Directive) that the vehicle’s owner or driver is automatically liable for any injury or damage caused by the vehicle, regardless of fault ([ AI liability in Europe | Ada Lovelace Institute](https://www.adalovelaceinstitute.org/resource/ai-liability-in-europe/#:~:text=strictly liable for any death%2C,as amended)%2C11 the)). This ensures an accident victim can be compensated via the owner’s compulsory motor insurance ([ AI liability in Europe | Ada Lovelace Institute](https://www.adalovelaceinstitute.org/resource/ai-liability-in-europe/#:~:text=strictly

Because of this baseline, if an ADAS-equipped car in Europe crashes, the injured party will typically claim against the driver’s insurer, even if the driver blames the technology. The driver (or their insurer) could then seek recourse from the manufacturer under product liability law. The EU Product Liability Directive (85/374/EEC) provides a strict liability regime for defective products: a carmaker (and other producers in the supply chain) can be held liable for damage caused by a defect in the vehicle or its components without the victim needing to prove negligence ([ AI liability in Europe | Ada Lovelace Institute](https://www.adalovelaceinstitute.org/resource/ai-liability-in-europe/#:~:text=Product liability%3A liability of the,the EU by Directive 85%2F374%2FEEC)). An ADAS module that fails to perform with reasonable safety may constitute a “defective product.” For instance, if a forward collision avoidance system had a known flaw that led to a crash, the vehicle manufacturer (or the supplier of that system) could be strictly liable to those harmed. This shifts the burden off the user to some extent. However, proving a defect in complex software can be challenging. The EU is actively updating its liability framework (with a forthcoming AI Liability Directive and a revised Product Liability Directive) to account for software and AI-driven features, aiming to prevent under-compensation of victims when autonomous or semi-autonomous technologies are involved ([ AI liability in Europe | Ada Lovelace Institute](https://www.adalovelaceinstitute.org/resource/ai-liability-in-europe/#:~:text=1. Avoiding under,parties)). These reforms may ease the burden of proof on plaintiffs in ADAS-related crashes (for example, potentially requiring manufacturers to disprove fault in certain cases).

Notably, Germany has been a pioneer in adapting road traffic law for higher levels of automation. Germany amended its Road Traffic Act in 2017 (and again in 2021) to address Level 3 and Level 4 vehicles. Under these rules, Level 3 automated driving is permitted, but the human in the driver’s seat is still legally considered the “driver” and must take over when prompted ([Liability Perspective for Users of Autonomous Vehicles in the EU - RAILS - Blog](https://blog.ai-laws.org/liability-perspective-for-users-of-autonomous-vehicles-in-the-eu/#:~:text=level 3 and level 4,party in level 4 vehicles)) ([Liability Perspective for Users of Autonomous Vehicles in the EU - RAILS - Blog](https://blog.ai-laws.org/liability-perspective-for-users-of-autonomous-vehicles-in-the-eu/#:~:text=The obligations of the driver,necessary%2C initiate necessary traffic safety)). A Level 3 user in Germany has a duty to monitor for takeover requests and regain control whenever the system reaches its limits ([Liability Perspective for Users of Autonomous Vehicles in the EU - RAILS - Blog](https://blog.ai-laws.org/liability-perspective-for-users-of-autonomous-vehicles-in-the-eu/#:~:text=The

United Kingdom
The UK has recently taken a groundbreaking step in reallocating accident liability for higher automation. In 2024, the Automated Vehicles Act (building on the earlier Automated & Electric Vehicles Act 2018) came into force, formally shifting liability from human drivers to manufacturers/vehicle software providers when a “self-driving” system is engaged ([UK law makes OEMs responsible for self-driving crashes | Traffic Technology Today](https://www.traffictechnologytoday.com/news/autonomous-vehicles/uk-law-makes-oems-responsible-for-self-driving-crashes.html#:~:text=The Automated Vehicles ,driving systems are engaged)). Under this law, if a vehicle is approved as having self-driving capability (as determined by UK regulators and listed accordingly) and it is operating in autonomous mode at the time of a crash, then the insurer (and ultimately the manufacturer) is liable for damage, not the human occupant ([UK law makes OEMs responsible for self-driving crashes | Traffic Technology Today](https://www.traffictechnologytoday.com/news/autonomous-vehicles/uk-law-makes-oems-responsible-for-self-driving-crashes.html#:~:text=The

It’s important to note that this UK regime applies only to vehicles or modes deemed “self-driving” by the Secretary of State. Current commercially available ADAS (Level 2 and most Level 3) are not yet on that list because they still rely on human fallback. For example, a Tesla in Autopilot mode or a Level 3 Mercedes-Benz “Drive Pilot” operating on a motorway might not be considered fully self-driving under the law’s criteria, meaning the human is still the responsible driver. Indeed, UK authorities (following recommendations of the Law Commission) distinguish between “driver assistance” and “self-driving” features. Level 2 and conditional Level 3 systems, where the human is expected to retake control, would likely be treated as driver assistance – so the human remains liable if misused. On the other hand, if a vehicle like a Level 3 Drive Pilot is approved for “automated” operation (e.g. traffic jam chauffeur up to 60 km/h), the intent is to apply the new Act’s provisions during those automated episodes. The practical effect as we stand in 2025 is that UK law is prepared to transfer liability to manufacturers once vehicles reach the point of safe self-driving capability. Until then, in day-to-day ADAS incidents, UK courts would still fall back on conventional principles: the driver owes a duty to control the car and could be found negligent if, say, they relied on lane centering and failed to prevent a crash. However, the UK’s statutory innovation ensures that as automation advances (Level 3+), liability will shift in a clear-cut way rather than leaving a gray zone. This provides a model framework for other jurisdictions to consider as they update laws for the “mushy middle” of automation.

Case Law: Defining Responsibility Between Driver and Automation
Though the technology is relatively new, several legal cases and investigations have already tested how responsibility is divided between human drivers and ADAS. These cases provide insight into how courts and regulators view the “shared control” paradigm:

Criminal Liability for ADAS Drivers: The first instances of criminal charges stemming from ADAS use have affirmed that a human driver can be held responsible even when automation is active. In People v. Kevin Riad (California, 2021), prosecutors charged a Tesla driver with two counts of vehicular manslaughter after his Model S, reportedly on Autopilot, ran a red light at high speed and caused a fatal crash. In 2023, Riad pled no contest to vehicular manslaughter – effectively accepting criminal responsibility ([Level 3 Automated Vehicles and Criminal Law - JURIST - Commentary - Legal News & Commentary](https://www.jurist.org/commentary/2023/08/widen-koopman-automated-vehicles-criminal-law/#:~:text=Current events focus a spotlight,under driver supervision%2C was engaged)) ([Level 3 Automated Vehicles and Criminal Law - JURIST - Commentary - Legal News & Commentary](https://www.jurist.org/commentary/2023/08/widen-koopman-automated-vehicles-criminal-law/#:~:text=In both cases%2C automation controlled,in sentencing without jail time)). Similarly, in Arizona, the backup safety driver of an Uber self-driving test vehicle faced charges (and later pleaded guilty to endangerment) after the vehicle, which was on Level 4 automated mode, struck and killed a pedestrian in 2018 ([Level 3 Automated Vehicles and Criminal Law - JURIST - Commentary - Legal News & Commentary](https://www.jurist.org/commentary/2023/08/widen-koopman-automated-vehicles-criminal-law/#:~:text=Current
Product Liability and Negligence Suits Against Manufacturers: On the civil side, injured parties have begun to test the liability of vehicle makers for crashes involving ADAS. A notable early case is Walter Huang (Tesla Autopilot crash), stemming from a 2018 incident in which a Tesla Model X on Autopilot crashed into a highway divider in Mountain View, California, killing the driver. Huang’s family sued Tesla, alleging that Autopilot was defective (it steered the car into the barrier with no warning) and that Tesla misrepresented its capabilities. That case (as of 2025) is still in litigation, highlighting the complicated questions of design defects and driver responsibility. Another case, Molander v. Tesla (mentioned earlier), went to a jury in 2023: the plaintiffs argued that an Autopilot defect caused a fatal crash, but the jury found no manufacturing defect and thus no liability for Tesla ([The Verdict Is In: California Jury Finds That Tesla’s “Autopilot” Feature Not Responsible For Fatal Accident | Knowledge | Fasken](https://www.fasken.com/en/knowledge/2023/11/california-jury-finds-that-teslas-autopilot-feature-not-responsible-for-fatal-accident#:~:text=After a nearly month,— the jury answered “no)). While Tesla has avoided liability in court so far, these trials illustrate the arguments. Plaintiffs often point to things like (a) inadequate warnings – e.g. Tesla branding its system “Autopilot” or “Full Self-Driving” could mislead drivers into overtrusting it, or (b) specific technical failures – e.g. the system misreading road conditions. Manufacturers, in defense, highlight that drivers are warned to stay attentive and that many crashes are due to driver misuse (such as ignoring repeated “hands on wheel” alerts ([Tesla driver in fatal 'Autopilot' crash got numerous warnings: U.S. government | Reuters](https://www.reuters.com/article/technology/tesla-driver-in-fatal-autopilot-crash-got-numerous-warnings-us-government-idUSKBN19A2XC/#:~:text=During a 37,NTSB said in the report)) ([Tesla driver in fatal 'Autopilot' crash got numerous warnings: U.S. government | Reuters](https://www.reuters.com/article/technology/tesla-driver-in-fatal-autopilot-crash-got-numerous-warnings-us-government-idUSKBN19A2XC/#:~:text=The report said the Autopilot,Hands Required Not Detected))). The outcome of such cases can hinge on evidence like vehicle data logs (to see if the system was engaged and how it performed) and expert testimony on whether the ADAS behaved reasonably. No consensus precedent has yet been set; some cases have settled out of court. What is clear is that if an automaker’s ADAS is found to have a defect that directly causes an accident, the automaker can be held liable under familiar product liability principles – just as an automaker would be if, say, brakes or steering failed due to a defect. ADAS introduces questions of software defects and human-machine interaction, which courts are handling on a fact-specific basis.
Insurance and Subrogation Cases: Because most vehicle accident claims are paid by insurance, an emerging dynamic is insurers paying out claims for ADAS-related crashes and then seeking to recover from manufacturers. For instance, if a car on adaptive cruise control rear-ends another because the system didn’t stop, the driver’s insurer might cover the victim’s damages but later sue the automaker, arguing the system was defective. One interesting lawsuit involved a commercial truck crash where the truck lacked available collision-avoidance technology; the trucking company settled claims and then there was litigation against the manufacturer for not installing a known safety feature (essentially a negligence claim for omission) ([Trucking Liability Claims: Collision Avoidance & Crash Mitigation](https://martinwrenlaw.com/blog/trucking-product-liability-claims-collision-avoidance-crash-mitigation/#:~:text=The first type of claims,mitigation technology in their trucks)) ([Trucking Liability Claims: Collision Avoidance & Crash Mitigation](https://martinwrenlaw.com/blog/trucking-product-liability-claims-collision-avoidance-crash-mitigation/#:~:text=But the problem is%2C most,that costs thousands of dollars)). These kinds of cases are prompting manufacturers to more widely include ADAS as standard equipment to avoid liability for missing features. As ADAS becomes ubiquitous, we may see insurers more aggressively treat system failures as product defects and demand contribution from manufacturers. However, those remain complex battles requiring proof that the technology should have prevented the crash.
Case Law in Europe/UK: In contrast to the U.S., there have been few public court cases in Europe solely about Level 2/3 ADAS crashes, likely because of the insurance structure (victims get paid by insurance and product-liability suits are rarer). One notable European case was not about a crash per se but about misleading advertising: a German court in 2020 ruled that Tesla’s use of the term “Autopilot” in marketing could mislead consumers about the car’s capabilities, and thus it banned certain advertising phrases. While not a tort case, this highlights regulatory attention on ensuring drivers are not over-sold on ADAS functionality. In the UK, as true self-driving cars deploy (likely in 2026 and beyond per the new Act ([UK law makes OEMs responsible for self-driving crashes | Traffic Technology Today](https://www.traffictechnologytoday.com/news/autonomous-vehicles/uk-law-makes-oems-responsible-for-self-driving-crashes.html#:~:text=automotive manufacturers when self,are engaged))), we will eventually see test cases of the new liability framework – e.g. an insurer suing a vehicle manufacturer after paying a claim in autonomous mode. For now, UK case law also mostly reinforces driver accountability. For example, in ordinary accidents, British courts have found drivers negligent for relying on aids (one case found a driver 100% liable for a crash despite claiming their car’s assist features didn’t work – the court said the driver should have been in control regardless).
In summary, early case law indicates that courts are unwilling to let drivers escape responsibility simply because they were “assisted” by automation. The human driver is still expected to act as a safety net for the technology. At the same time, manufacturers are beginning to be held to account for the promises and performance of their ADAS – if a company advertises that a system will “prevent” collisions or allows it to operate in conditions it cannot handle, courts and regulators view that critically. This dual scrutiny is shaping a nascent jurisprudence: shared control can mean shared liability, but establishing the shares (driver vs. manufacturer) depends on the specifics of the case – was the driver inattentive or misusing the system, and did the system function as designed or did it fail to meet reasonable expectations?

Common Accident Scenarios and Liability Interpretation
Different crash scenarios highlight how liability is parsed when ADAS is involved. We consider a few common accident types and how responsibility is currently assigned:

Rear-End Collisions
Scenario: A car with adaptive cruise control and AEB fails to stop in time and rear-ends a lead vehicle. Rear-end crashes are a classic case of human-driver fault: traffic law almost always presumes the trailing driver is at fault for not maintaining a safe distance. Does ADAS change this? If a driver was relying on ACC/AEB and it didn’t prevent the crash, legal fault still initially falls on the trailing driver. The rationale is that the driver should have been monitoring and should have braked if the system did not. From an insurance perspective, the claim is paid by the trailing car’s liability coverage in most jurisdictions. However, the driver might argue that the technology was defective – potentially shifting liability via a product claim. For example, if evidence shows the AEB never engaged despite an obvious obstacle, the driver (or their insurer) could pursue the manufacturer, asserting that a defect in the collision avoidance system caused the crash ([ADAS and Car Accident Liability: What You Need to Know](https://stokesstemle.com/car-accident-lawyer/adas-and-car-accident-liability-what-you-need-to-know/#:~:text=,in the ADAS’s computer code)) ([ADAS and Car Accident Liability: What You Need to Know](https://stokesstemle.com/car-accident-lawyer/adas-and-car-accident-liability-what-you-need-to-know/#:~:text=vehicle

Lane Departure and Sideswipe Crashes
Scenario: A vehicle drifts out of its lane or makes an unintended lane change, striking another vehicle or a roadside object, despite having lane-keeping assistance and blind spot warnings. Here, the immediate cause is the vehicle leaving its proper lane – something a prudent driver should not allow. If a driver’s car veers into oncoming traffic or off the road, the driver will likely be deemed negligent (for inattention or drowsiness) in the absence of other evidence. ADAS like LKA is designed to prevent these accidents by issuing alerts or gently steering, but it’s not foolproof. Legal interpretation: If a lane departure crash happens, authorities will ask: Did the driver ignore a lane departure warning? If yes, liability clearly stays with the driver (they failed to respond). If the LKA failed to activate entirely, the driver is still on the hook to maintain control, though they might later claim the system’s failure worsened the situation. Imagine a case where a car slowly drifts because the driver was momentarily distracted, and the LKA neither warned nor corrected; the car hits a guardrail. Police would fault the driver for not paying attention. The driver could in turn look to the automaker: was there a defect in the lane-keeping system? If a defect is proven (say the system was meant to warn but didn’t due to a sensor issue), the manufacturer might bear some liability for the damage to the car or any third-party injuries. Another variant is when using semi-automated lane changing. Some Level 2 systems allow the car to perform a lane change when the driver signals, or even automatically (as Tesla’s Navigate on Autopilot attempts). Regulations are clear that in Level 2, the driver must supervise these maneuvers ([Drivers to be responsible for lane changes made by driver assistance systems - ETSC](https://etsc.eu/drivers-to-be-responsible-for-lane-changes-made-by-driver-assistance-systems/#:~:text=New regulations under development at,not make the manoeuvre themselves)). If an automated lane change results in a sideswipe collision (perhaps the system didn’t see a fast-approaching car in the next lane), the driver is usually deemed at fault for an improper lane change. The fact that the computer executed it doesn’t exempt the driver; legally it’s as if the driver made a bad lane change. The ETSC (European Transport Safety Council) has voiced concern that allowing automated lane changes without explicit driver input could confuse this issue, but current policy is to still hold the driver accountable for anything a Level 2 system does ([Drivers to be responsible for lane changes made by driver assistance systems - ETSC](https://etsc.eu/drivers-to-be-responsible-for-lane-changes-made-by-driver-assistance-systems/#:~:text=New

Intersection and Crossing-Path Crashes
Scenario: A Level 2/3 equipped car goes through an intersection and collides with another vehicle or a pedestrian – for example, running a red light or failing to yield to crossing traffic – while the driver had the automation engaged. Intersections are complex environments often beyond the capability of Level 2 systems (most ADAS do not handle traffic lights or cross-traffic unless the driver is actively in control). A prominent example is the 2016 Tesla crash where Autopilot did not recognize a truck turning across its path at an intersection, resulting in a fatal impact ([NTSB: 'Operational Limitations' Played Major Rule in Tesla Autopilot Crash - Business Insider](https://www.businessinsider.com/r-ntsb-operational-limitations-played-major-rule-in-tesla-autopilot-crash-2017-9#:~:text=WASHINGTON (Reuters) ,autonomous "Autopilot" system)). In such cases, current liability practice squarely places fault on the driver for failing to obey traffic signals or yield rights-of-way. Automated driving modes like Tesla’s “Full Self-Driving (Beta)” that attempt to handle intersections are still supervised (Level 2), so legally the user must monitor and intervene to obey traffic laws. If the car runs a red light, it’s the driver who has committed a violation, even if they claim the car didn’t alert them. Indeed, in the California manslaughter case, the fact that Autopilot was supposedly active when it ran the red light did not prevent the driver’s prosecution ([Level 3 Automated Vehicles and Criminal Law - JURIST - Commentary - Legal News & Commentary](https://www.jurist.org/commentary/2023/08/widen-koopman-automated-vehicles-criminal-law/#:~:text=system (ADS) was engaged,under driver supervision%2C was engaged)). From a civil liability perspective, any harm to others (cross-traffic vehicles, pedestrians) will be attributed to the “driver running a light/stop.” Can the driver avoid blame by pointing to the machine? Generally not in Level 2. They might have a claim against the manufacturer if the system was supposed to help and failed – for instance, if an automated emergency braking should have detected cross traffic or a pedestrian and didn’t. But many ADAS are currently not designed for intersection scenarios; manufacturers will note that (and indeed they often warn “the system may not detect vehicles crossing your path”). Thus, it’s hard to paint it as a defect if the system simply wasn’t intended to handle that case. For Level 3 systems (which handle driving in some conditions), one could imagine a scenario: say a Level 3 car in traffic jam pilot mode comes to an intersection and behaves inappropriately without alerting the driver in time. If a crash occurs, liability could hinge on whether the system should have handed control back to the human earlier. This is a gray area regulators are still working out. For now, however, virtually all intersection collisions involving ADAS have been treated as human error – the human either engaged the system in an environment it wasn’t meant for or failed to take over when approaching the intersection. Manufacturers have so far avoided liability for these events by citing misuse or the limitations of current technology. Going forward, as systems improve, if a car on “autonomous mode” drives through an intersection and crashes, then we will test the new legal waters (e.g. did the car’s AI make a mistake for which the manufacturer is liable, or should the occupant have been supervising?). Until Level 3+ is common, intersection accidents will usually be litigated as standard traffic accidents (driver ran light/stop sign or failed to yield). The presence of ADAS might be a footnote unless the plaintiff can show a malfunction (like the car started moving on its own when it shouldn’t have, which would indeed implicate the maker). In summation, context is king – current ADAS are not trusted legally to manage intersections, so any crash in such a context reverts liability to the driver’s actions or inactions.

Private Drivers vs. Commercial Fleets: Liability Considerations
The implications of ADAS accidents can differ for private individuals versus commercial fleet operators due to differences in use patterns, insurance, and regulatory scrutiny:

Private Drivers: For individual car owners, an ADAS-related accident typically follows the patterns described above. The driver’s personal auto insurance will cover damages (subject to policy terms), and that insurer may later investigate if the ADAS malfunctioned. Private drivers might face higher hurdles in pursuing a product liability case against a manufacturer, given the resources required, but there have been instances (as with Tesla owners) where individuals or families do bring lawsuits for alleged system failures. Automakers often settle such claims quietly if merit is shown, to avoid setting a public precedent. Another consideration is driver education – many private owners may not fully understand their ADAS’s limits. If a crash occurs because an owner over-relied on the system, a manufacturer might point to the owner’s manual disclaimers and driver error ([The Verdict Is In: California Jury Finds That Tesla’s “Autopilot” Feature Not Responsible For Fatal Accident | Knowledge | Fasken](https://www.fasken.com/en/knowledge/2023/11/california-jury-finds-that-teslas-autopilot-feature-not-responsible-for-fatal-accident#:~:text=suite of advanced driver,take over at any time)). From a liability standpoint, nothing fundamentally separates a private driver’s responsibility: they are expected to know their vehicle’s features and use them responsibly. However, there is a consumer protection angle: if an average consumer could not reasonably comprehend the limits (due to confusing marketing or interface), a court might find the manufacturer failed in its duty to warn. In sum, private drivers largely bear the brunt of liability unless they can prove the car was to blame.
Commercial Fleets: Commercial use of ADAS (in trucking, ridesharing, company car fleets, etc.) introduces additional layers of liability. A commercial driver operating a truck with ADAS who crashes could make their employer and others liable under vicarious liability and other theories. For instance, if a truck driver using adaptive cruise control rear-ends someone, not only is the driver liable, but the trucking company (employer) will typically be held vicariously liable to the victim. The company’s insurer pays out. Now, that company might in turn look at the vehicle or technology supplier if a failure occurred. Large fleet operators are already attuned to product issues – e.g. if a collision avoidance system failed, the fleet may engage in a product liability claim or join a class action against the OEM. Moreover, commercial entities have a duty to train their drivers on new technology. If a company failed to train a driver on the proper use of, say, a lane-keeping aid, and misuse led to a crash, the company could be directly negligent. Conversely, a well-advised fleet will instruct drivers that these systems are assists only, and maintain the systems diligently. Maintenance is another factor: commercial vehicles accumulate high mileage, and ADAS sensors require calibration and upkeep. If a fleet neglects sensor calibration (say after a windshield replacement on a car with a camera-based LKA) and a crash happens, liability might point to the fleet’s poor maintenance. Thus, commercial operators must manage not just driver behavior but also the tech’s maintenance to avoid liability.
Regulatory and Insurance Differences: Commercial vehicles often carry higher insurance limits and face regulatory oversight (for example, federal safety regulations for trucking mandate certain safety equipment). There is movement toward requiring ADAS on new heavy trucks (e.g. automatic emergency braking will be mandated in the EU for trucks, and NHTSA has proposed AEB rules for U.S. trucks). Once a technology is mandated, failure to have it (or disabling it) could expose a company to liability per se. Already, there was a case where plaintiffs sued a truck manufacturer for not including AEB as standard, arguing the truck was defective without it ([Trucking Liability Claims: Collision Avoidance & Crash Mitigation](https://martinwrenlaw.com/blog/trucking-product-liability-claims-collision-avoidance-crash-mitigation/#:~:text=The reality is that crash,technology was introduced in 2007)) ([Trucking Liability Claims: Collision Avoidance & Crash Mitigation](https://martinwrenlaw.com/blog/trucking-product-liability-claims-collision-avoidance-crash-mitigation/#:~:text=When manufacturers opt not to,of not having the technology)). The case settled, but it sends a signal – what is optional today might be seen as essential tomorrow, and not using available safety tech could be deemed negligent. Commercial fleets also tend to adopt telematics and driver monitoring; if an ADAS alert notified the fleet’s operations center that a driver was inattentive and the fleet did nothing, that could be considered in litigation. On the flip side, commercial users may benefit from clearer contracts with manufacturers. For example, a delivery company buying semi-automated vans might have agreements on liability or service if the system fails.
Shared Mobility (Rental and Rideshare): If a rideshare vehicle or a rental car has ADAS that misbehaves, the question arises: could the platform or rental company be liable? Generally, if a rideshare driver is using their own car with ADAS and crashes, it’s like any private driver scenario (with the wrinkle that the rideshare company’s insurance may cover some damages). Rental car companies typically disclaim liability for how renters drive; a renter who relies too much on ADAS and crashes would be liable just as if it were their own car. The rental company might seek indemnity from the manufacturer if a defect is proven. These situations haven’t been extensively tested yet.
In summary, commercial fleets have more at stake – they face potential employer liability for driver mistakes and have the clout to pursue manufacturers if technology fails. They are incentivized to enforce proper use of ADAS through training and policy. For private individuals, the consequences of misusing ADAS are more personal (injury, liability, insurance surcharges), and their recourse against manufacturers is limited but not absent. Both private and commercial users ultimately highlight the same legal point: as long as a human is expected to share driving responsibility, that human (or their employer) will be a focal point of liability when things go wrong.

Conclusion and Recommendations
Bridging the Gap: The current legal treatment of accidents involving Level 2–3 automation reveals a gap between technology and law. These systems operate in a gray zone – they handle driving tasks most of the time, but legally the human is still “in charge.” This ambiguity can lead to uncertainty and litigation over who is to blame in a crash. To foster both the adoption of life-saving technology and the fair allocation of responsibility, regulators and lawmakers should provide clearer guidelines. As automation advances, existing liability frameworks will need adjustment, because simply blaming the human in all cases may not always lead to just outcomes (nor promote public trust in automation) ([Level 3 Automated Vehicles and Criminal Law - JURIST - Commentary - Legal News & Commentary](https://www.jurist.org/commentary/2023/08/widen-koopman-automated-vehicles-criminal-law/#:~:text=Primarily with respect to civil,the new ways of driving)). Likewise, manufacturers need consistent rules so they can manage risk and innovate responsibly ([Level 3 Automated Vehicles and Criminal Law - JURIST - Commentary - Legal News & Commentary](https://www.jurist.org/commentary/2023/08/widen-koopman-automated-vehicles-criminal-law/#:~:text=Law reform should clarify several,state legislatures because certainty gives)) ([Level 3 Automated Vehicles and Criminal Law - JURIST - Commentary - Legal News & Commentary](https://www.jurist.org/commentary/2023/08/widen-koopman-automated-vehicles-criminal-law/#:~:text=Primarily

Proposed Regulatory Guidelines:

1. Clarify Driver Duties at Level 2 and Level 3: Regulations should explicitly codify the responsibilities of drivers when using ADAS. This could include requiring driver oversight at all times for Level 2 (which is already the case de facto) and defining what “attentiveness” means (e.g. mandating driver-monitoring cameras to ensure eyes on road). For Level 3 (conditional automation), laws must clarify when a driver is not responsible. For instance, if a Level 3 system is engaged and driving within its approved operational domain, the law could state the human is not liable for traffic infractions or initial collision causation during that time. However, once a takeover request is issued, the human should regain liability after a reasonable grace period. A specific guideline – such as the 10-second rule from the UN’s ALKS regulation ([Level 3 Automated Vehicles and Criminal Law - JURIST - Commentary - Legal News & Commentary](https://www.jurist.org/commentary/2023/08/widen-koopman-automated-vehicles-criminal-law/#:~:text=However%2C as part of the,is expected to assume control)) ([Level 3 Automated Vehicles and Criminal Law - JURIST - Commentary - Legal News & Commentary](https://www.jurist.org/commentary/2023/08/widen-koopman-automated-vehicles-criminal-law/#:~:text=Second%2C there is the thorny,time of a takeover request)) – could be adopted: after an alert, the driver has, say, 10 seconds to respond before being considered at fault for subsequent incidents. If a crash happens within that grace period, and the system hadn’t already minimized risk, the manufacturer or vehicle should bear primary responsibility ([Level 3 Automated Vehicles and Criminal Law - JURIST - Commentary - Legal News & Commentary](https://www.jurist.org/commentary/2023/08/widen-koopman-automated-vehicles-criminal-law/#:~:text=Second%2C
2. Manufacturer Liability for Verified System Failures: To encourage safe design, regulators should affirm that when an ADAS or automated driving system malfunctions or performs outside reasonable expectations, manufacturers (or AV software developers) will be held liable. This could be through strengthened product liability statutes or new “strict liability for automated systems” provisions. For example, if a Level 3 vehicle running in automated mode breaks traffic laws (exceeds speed limit egregiously or runs a red light) without handing control back to the user, the law should presumptively assign fault to the automated driving system/provider ([Level 3 Automated Vehicles and Criminal Law - JURIST - Commentary - Legal News & Commentary](https://www.jurist.org/commentary/2023/08/widen-koopman-automated-vehicles-criminal-law/#:~:text=Third%2C there is the question,and caused a fatal crash)) ([Level 3 Automated Vehicles and Criminal Law - JURIST - Commentary - Legal News & Commentary](https://www.jurist.org/commentary/2023/08/widen-koopman-automated-vehicles-criminal-law/#:~:text=cases for which an operator,and caused a fatal crash)). The UK approach already envisions this by shifting liability to the company when self-driving mode is active ([UK law makes OEMs responsible for self-driving crashes | Traffic Technology Today](https://www.traffictechnologytoday.com/news/autonomous-vehicles/uk-law-makes-oems-responsible-for-self-driving-crashes.html#:~:text=The Automated Vehicles ,driving systems are engaged)). Other jurisdictions might implement a similar scheme for any certified Level 3–4 functionalities. In practice, this means if a car is driving itself and causes harm, the victim doesn’t have to prove negligence; the manufacturer is automatically responsible (except in cases of blatant misuse or tampering by the user).
3. Data Recording and Transparency: Regulations should require robust event data recorders (EDRs) and automated driving logs in vehicles with ADAS. In disputed crashes, data on whether the system was engaged, whether it issued alerts, and how it performed is crucial. Standardizing this (as UN Regulation 157 does by mandating data storage for ALKS) helps assign liability fairly. Moreover, manufacturers must be transparent with this data. A regulatory body could hold this data in escrow to facilitate investigations. This will prevent situations where a driver and a manufacturer blame each other without evidence; the car’s logs should tell the story (e.g. “Autopilot was engaged and gave a warning 5 seconds before impact, which the driver ignored” versus “the system gave no warning and did not detect the obstacle”).
4. Driver Training and Public Education: One non-legal but important measure is improving driver understanding of ADAS. Misuse and over-trust are leading causes of ADAS-involved accidents ([NTSB: 'Operational Limitations' Played Major Rule in Tesla Autopilot Crash - Business Insider](https://www.businessinsider.com/r-ntsb-operational-limitations-played-major-rule-in-tesla-autopilot-crash-2017-9#:~:text=WASHINGTON (Reuters) ,autonomous "Autopilot" system)). Licensing agencies should update driver exams and materials to cover ADAS use. Perhaps require drivers to demonstrate knowledge of their specific vehicle’s ADAS before an insurer grants a discount or coverage for using it. While this doesn’t directly change liability rules, it addresses root causes and may reduce the incidence of crashes (and thus liability conflicts). Some jurisdictions might even mandate that dealers provide a hands-on ADAS orientation to buyers.
5. Update Traffic Laws to Recognize “Automated Mode”: Traffic codes could include definitions such as “automated driving mode engaged” and specify how enforcement works. For example, if a police officer sees a car commit a violation and it’s in Level 3 mode, the law could require citing the vehicle owner/registrant rather than the individual in the driver’s seat (similar to how we treat automated traffic enforcement or even how the UK will treat automated vehicles). This notion ties into insurance: ensure insurance policies explicitly cover automated operation and delineate when the insurer can subrogate against a manufacturer. In the interim, states in the U.S. and countries in Europe should ensure their compulsory insurance schemes don’t leave victims in limbo; no one should be left uncompensated because of finger-pointing between a driver and a tech company.
6. Encourage Voluntary Assumption of Liability by Automakers: Policymakers should encourage manufacturers to stand behind their ADAS. Notably, some automakers have publicly declared they will accept liability when their cars are in self-driving mode (e.g. Volvo’s pledge to accept full liability for its future autonomous cars) ([Volvo Will Accept Liability For Its Self-Driving Cars - Forbes](https://www.forbes.com/sites/jimgorzelany/2015/10/09/volvo-will-accept-liability-for-its-self-driving-cars/#:~:text=Volvo Will Accept Liability For,”)). Mercedes-Benz similarly announced it would assume liability for crashes caused by its Level 3 Drive Pilot system under certain conditions ([Mercedes-Benz's Drive Pilot: Is liability still a roadblock? - PreScouter](https://www.prescouter.com/2024/04/mercedes-benz-level-3-drive-pilot/#:~:text=Mercedes,the autonomous system is active)) ([DrivePilot Autonomous Driving System: What it Means for Drivers ...](https://www.egletlaw.com/mercedes-benz-level-3-drivepilot-system-what-it-means-for-drivers-and-accident-victims-in-nevada/#:~:text=DrivePilot Autonomous Driving System%3A What,legal liability for injuries)). Such commitments are reassuring but need legal teeth. Regulators could make liability acceptance a condition of deploying higher-level automation: if a company wants to offer a Level 3 system, it must guarantee to cover at-fault accidents caused by system errors. This aligns incentives – companies will rigorously vet their tech if they know they pay for its failures.
7. International Harmonization: As vehicles cross borders, a patchwork of liability rules could impede deployment. International bodies (UNECE, EU, etc.) should continue efforts to harmonize standards, including liability and insurance norms for automated driving. For instance, a unified approach to data recording and a baseline of manufacturer responsibility would prevent confusion if a crash occurs in a different jurisdiction than where the car is registered. The Vienna Convention amendments and EU regulations are a start, but more alignment is needed on liability so that a Level 3 car isn’t considered “driver-in-charge” in one country but “system-in-charge” in another.
In conclusion, accidents involving shared control of human and ADAS challenge our traditional notions of driver liability. Presently, the pendulum still swings toward blaming the human driver in Level 2 and 3 scenarios – a reflection of the law’s caution and the technology’s limits. However, as vehicles inch closer to full self-driving, accountability must progressively shift to the party that truly controls the driving task. The U.S., EU, and UK are each navigating this shift, with the UK forging an explicit path to manufacturer liability for self-driving systems ([UK law makes OEMs responsible for self-driving crashes | Traffic Technology Today](https://www.traffictechnologytoday.com/news/autonomous-vehicles/uk-law-makes-oems-responsible-for-self-driving-crashes.html#:~:text=The Automated Vehicles ,driving systems are engaged)). Policy-makers and courts will need to refine rules so that neither victims nor innovation suffer: drivers should not be unfairly penalized for system failures, and manufacturers should not escape liability when their technology is effectively at the helm ([Level 3 Automated Vehicles and Criminal Law - JURIST - Commentary - Legal News & Commentary](https://www.jurist.org/commentary/2023/08/widen-koopman-automated-vehicles-criminal-law/#:~:text=Law reform should clarify several,state legislatures because certainty gives)) ([Level 3 Automated Vehicles and Criminal Law - JURIST - Commentary - Legal News & Commentary](https://www.jurist.org/commentary/2023/08/widen-koopman-automated-vehicles-criminal-law/#:~:text=Primarily with respect to civil,the new ways of driving)). By implementing clear guidelines on driver responsibilities, mandating transparency and safety assurances from manufacturers, and updating legal definitions, we can ensure that the liability framework keeps pace with technological reality. The goal is a future where advanced driving systems improve safety without creating uncertainty about legal responsibility – a future where both drivers and developers know the rules of the road, both physically and legally, when it comes to shared driving control.